{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b74e6e22-abe0-4775-8791-e951c9948dce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Objective: Understand the layered architecture of modern data platforms.\n",
    "\n",
    "Dataset: ecommerce_orders.csv\n",
    "\n",
    "### Tasks:\n",
    "1. Split this raw dataset logically into:\n",
    "\n",
    "    -> Bronze Layer: Raw ingestion (entire dataset)\n",
    "\n",
    "    -> Silver Layer: Only successful (status = delivered) orders\n",
    "    \n",
    "    -> Gold Layer: Aggregated metrics like total revenue per product\n",
    "\n",
    "2. Create a markdown visual representation (table or diagram) of how data flows from\n",
    "Bronze → Silver → Gold.\n",
    "\n",
    "3. Write one paragraph answering:\n",
    "“Why is the Lakehouse model better than traditional Data Warehousing for modern companies?”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efd09d36-6338-4ce9-92fc-6c380a4d3ad5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_path = '/Workspace/Users/akashkchavan9900@gmail.com/data-engineering-projects/Datasets/ecommerce_orders.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d059b88-dda1-43e8-ae78-d524cfc38a9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 1: \n",
    "Bronze Layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd7e8c07-a86f-49b0-a057-a22066d22759",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21275ddf-669e-4bc5-aabe-acb686800b24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "SIlver Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ac78cc0-eb33-4b0e-94f5-cda88c1e6b9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = df[df['status'] == 'delivered']\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6357a48-81a7-43e7-b8be-cc7310709968",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Gold Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f294de5e-574d-4ee8-a26e-c67e5b83ffed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2 = df1.groupby('product', as_index=False)['amount'].sum()\n",
    "df2.rename(columns={'amount': 'total_revenue'}, inplace=True)\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0abb006-ac95-4435-a85a-19e7f0fea83d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 2: \n",
    "#### Data Flow: Bronze → Silver → Gold\n",
    "\n",
    "| Layer  | Description                                      | Data Criteria                            |\n",
    "|--------|--------------------------------------------------|------------------------------------------|\n",
    "| Bronze | Raw ingestion of all data                        | All records from the source              |\n",
    "| Silver | Cleaned and filtered data                        | Only where `status = delivered`         |\n",
    "| Gold   | Aggregated insights for analysis or reporting    | Revenue grouped by product              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb3ac4d4-060f-4ec8-a99c-cc6f19493b28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 3: \n",
    "#### Why is the Lakehouse model better than traditional Data Warehousing for modern companies?\n",
    "-> \n",
    "The Lakehouse model combines the best of both worlds—data lakes and data warehouses. Traditional data warehouses are great for structured analytics but fall short when dealing with large volumes of semi-structured or unstructured data. On the flip side, data lakes offer flexibility and scalability but often lack reliability for business-critical analytics. Lakehouse solves this by using open formats like Parquet and Delta Lake to provide a single storage layer for both structured and unstructured data, with support for ACID transactions, schema enforcement, and BI workloads. For modern companies, it means faster insights, lower data duplication, and reduced infrastructure costs—all in one place."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "M0_3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
